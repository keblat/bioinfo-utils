# optrimAl package
# Author: Shee, Zhi Qiang
# Conditions of use: GPLv3
# There are two scripts here - PASTA_taster.sh and optrimAl.R

# -------------------

#!/bin/bash
# PASTA_taster.sh - run this first

# This script tests if the pasta is gluten-free.
# JK. This is a wrapper around optrimAl that generates trimmed alignments using trimAl 1.2 (Capella-Gutierrez et al, 2009) at successively stricter trimming thresholds then summarises statistics for these trimmed alignments using AMAS (Borowiec, 2016).
# It calls the optrimAl script, which then returns all alignment files trimmed to an optimum threshold defined as yielding the maximum proportion of parsimony informative characters but losing no more data than one median absolute deviation above the median data loss across the entire range of trimming thresholds being tested.
# Alignments that lose more than a set cap of data (default 30% in the script) after optimal trimming are not returned.
# Theoretically, I prefer this approach because it considers the amount of missing data in each data set and avoids excessive trimming, instead of setting an arbitrary fixed gap threshold, which DOES result in loss of informativeness in some data sets.
# Empirically, this approach has NOT been tested.
# If you find any bugs, please modify accordingly and let me know (totally optional). I will probably not have any time to troubleshoot in the near future (maybe when I'm finally retired). Good luck! ZQ

# This script produces ALOT of output.
# Alignment files (e.g. *.aln) returned to the working directory are the optimally trimmed alignments.
# overlost.txt lists the alignments where data loss exceeded the cap.
# dldp*.png are graphs showing the proportion of parsimony informative characters and data loss at each trimming threshold, as well as the selected trimming threshold, for each alignment.
# dldp*.csv are the raw data from which the graphs are produced.
# summary*.txt are the summary statistics produced by AMAS.
# Directories named with the specified trimming threshold values (e.g. 0.1) should be deleted immediately once done with analysis as they take up ALOT of space.

# Provide a text file with desired trimming threshold values, one per line (cutoff_trim.txt).
# Make sure to set the working directory and trimAl path correctly, that optrimal.R and cutoff_trim.txt are in the same directory, update the file name pattern for the alignment where required and provide a set of trimming thresholds (any number of thresholds from 0 to 1, one threshold per line, must include 0 and 1) in the cutoff_trim.txt input file.
# My working directory in this case was ‘~/zq/working/optrimal’, my trimAl path was ‘~/zq/bin/trimAl/source/trimal’ and my file name pattern was 'g*' so just change those accordingly.
# This script WILL generate non-fatal errors where alignments are missing - check if these alignments were intentionally omitted or went missing for some other reason.

while read cutoff_trim
do
        cd ~/zq/working/optrimal
        mkdir $cutoff_trim

        for alignment in g*
        do
          ~/zq/bin/trimAl/source/trimal -in ${alignment}/output_alignment.fasta -out ${cutoff_trim}/${alignment}.aln -htmlout ${cutoff_trim}/${alignment}.htm -gt $cutoff_trim

                # check if alignment was trimmed to extinction by trimAl

                if grep ' 0 bp' ${cutoff_trim}/${alignment}.aln
                then
                        rm -f ${cutoff_trim}/${alignment}.aln
                fi
        done

        cd ~/zq/working/optrimal/${cutoff_trim}
        python3 ~/zq/bin/AMAS-master/amas/AMAS.py summary -f fasta -d dna -i *.aln

        mv summary.txt ../summary_${cutoff_trim}.txt

done < cutoff_trim.txt

xvfb-run Rscript –vanilla optrimal.R


# --------------------

# optrimAl.R - run this second

cutoff_trim <- readLines('cutoff_trim.txt')

# create one multiple tables for each threshold value to store AMAS results

amas_table <- read.table('summary_0.txt', header = TRUE)
sites <- data.frame(row.names = amas_table$Alignment_name)
pct <- data.frame(row.names = amas_table$Alignment_name)
filled <- data.frame(row.names = amas_table$Alignment_name)
lost <- data.frame(row.names = amas_table$Alignment_name)

for(i in 1:length(cutoff_trim)){
  amas_table <- read.table(paste('summary_', cutoff_trim[i], '.txt', sep = ''), header = TRUE)
  for(j in amas_table$Alignment_name){
    sites[rownames(sites) == j,i] <- amas_table$Parsimony_informative_sites[amas_table$Alignment_name == j]
    pct[rownames(pct) == j,i] <- as.numeric(amas_table$Proportion_parsimony_informative[amas_table$Alignment_name == j])
    filled[rownames(filled) == j,i] <- amas_table$Total_matrix_cells[amas_table$Alignment_name == j] * (1 - amas_table$Missing_percent[amas_table$Alignment_name == j] / 100)
  }
}

# calculate data loss for each trimming threshold

sites[is.na(sites)] <- 0
pct[is.na(pct)] <- 0

for(i in 1:ncol(filled)){
  lost[,i] <- 1 - filled[,i] / filled[,1]
}

lost[is.na(lost)] <- 1

colnames(sites) <- cutoff_trim
colnames(pct) <- cutoff_trim
colnames(filled) <- cutoff_trim
colnames(lost) <- cutoff_trim

# select optimal trimming threshold
# current criterion is maximum proportion of parsimony informative sites where data loss is no more than one median absolute deviation above the median

optrim <- numeric()
optrim_loss <- numeric()

for(i in rownames(pct)){
  lost_i <- unlist(lost[rownames(lost) == i, ])
  pct_i <- unlist(pct[rownames(pct) == i, ])
  dldp <- data.frame(pct_i, lost_i, row.names = cutoff_trim)
  write.csv(dldp, paste('dldp_', i, '.csv', sep = ''))
  real_loss <- dldp$lost_i[dldp$lost_i < 1]
  diff_loss <- real_loss[2:length(real_loss)] - real_loss[1:(length(real_loss) - 1)]
  median_loss <- median(diff_loss[diff_loss != 0])
  dldp <- subset(dldp, dldp$lost_i <= (median(real_loss) + median_loss))
  if(length(dldp$pct_i) > 0){
    optrim[i] <- rownames(dldp)[dldp$pct_i == max(dldp$pct_i)][[1]]
    optrim_loss[i] <- dldp$lost_i[rownames(dldp) == optrim[i][[1]]]
  } else {
    optrim[i] <- 0
    optrim_loss[i] <- 0
  }
}

# generate graphs to show effect of trimming on informativeness and data loss

for(i in rownames(pct)){
  dldp <- read.csv(paste('dldp_', i, '.csv', sep = ''))
  png(paste('dldp_', i, '.png', sep = ''))
  par(mar = c(5,5,2,5))
  plot(main = i, dldp$lost_i ~ cutoff_trim, ylim = c(0,1), ylab = 'proportion of data lost', xlab = 'strictness of trimming (trimAl gap threshold)', pch = 18, col = 'red')
  par(new = T)
  plot(dldp$pct_i ~ cutoff_trim, xlab = NA, ylab = NA, ylim = c(0,1), axes = F, pch = 16, col = 'blue')
  axis(side = 4)
  mtext(side = 4, line = 3, 'proportion parsimony informative')
  legend(x = 0, y = 1, legend = c('proportion of data lost', 'proportion of parsimony informative sites', 'selected trimming threshold'), pch = c(18, 16, NA), lty = c(NA, NA, 2), col = c('red', 'blue', 'black'), cex = 0.9, bty = 'n')
  if(is.na(optrim[i]) == FALSE){
    lines(c(-0.5, optrim[i]), c(optrim_loss[i], optrim_loss[i]), lty = 2)
    lines(c(-0.5, optrim[i]), c(dldp$pct_i[dldp$X == optrim[i]], dldp$pct_i[dldp$X == optrim[i]]), lty = 2)
    lines(c(optrim[i], optrim[i]), c(-0.5, max(optrim_loss[i], dldp$pct_i[dldp$X == optrim[i]])), lty = 2)
  }
  dev.off()
}

overlost <- names(optrim_loss[optrim_loss > 0.3])

write(overlost, 'overlost.txt', sep = '\n')

file.copy(paste(optrim, '/', names(optrim), sep = ''), getwd())

file.remove(paste(overlost, sep = '')) 
